{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install implicit\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcrsbsKP3-0j",
        "outputId": "3d4e31da-0be5-4011-e0d9-6ab03c9e1bae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting implicit\n",
            "  Downloading implicit-0.7.2.tar.gz (70 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/70.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from implicit) (2.0.2)\n",
            "Requirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.12/dist-packages (from implicit) (1.16.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from implicit) (4.67.1)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.12/dist-packages (from implicit) (3.6.0)\n",
            "Building wheels for collected packages: implicit\n",
            "  Building wheel for implicit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for implicit: filename=implicit-0.7.2-cp312-cp312-linux_x86_64.whl size=10797486 sha256=7a032fdafc26e8de768d7df0e5d581569cf1eec07ba866dcbdecdd8101508f63\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/00/4f/9ff8af07a0a53ac6007ea5d739da19cfe147a2df542b6899f8\n",
            "Successfully built implicit\n",
            "Installing collected packages: implicit\n",
            "Successfully installed implicit-0.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load your datasets\n",
        "meta = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/RS_Mini/meta_All_Beauty.csv\")\n",
        "reviews = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/RS_Mini/All_Beauty_5.csv\")\n",
        "\n",
        "# Keep relevant columns\n",
        "reviews = reviews[['reviewerID', 'asin', 'unixReviewTime']]\n",
        "meta = meta[['asin', 'title']]\n",
        "\n",
        "# Merge metadata\n",
        "df = reviews.merge(meta, on=\"asin\", how=\"left\")\n",
        "\n",
        "# Encode users and items\n",
        "df['user_idx'] = df['reviewerID'].astype('category').cat.codes\n",
        "df['item_idx'] = df['asin'].astype('category').cat.codes\n",
        "\n",
        "num_users = df.user_idx.nunique()\n",
        "num_items = df.item_idx.nunique()\n",
        "\n",
        "print(\"Users:\", num_users, \"| Items:\", num_items)\n",
        "\n",
        "# Sort by time\n",
        "df = df.sort_values(\"unixReviewTime\")\n",
        "\n",
        "# Train–test split: last interaction of each user = test\n",
        "test_df = df.groupby('user_idx').tail(1)\n",
        "train_df = df.drop(test_df.index)\n",
        "\n",
        "print(\"Train size:\", len(train_df), \" | Test size:\", len(test_df))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dffy6IA9Gxt1",
        "outputId": "097dbd24-ccff-4c93-f0de-1087404b9d06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Users: 991 | Items: 85\n",
            "Train size: 4776  | Test size: 991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BASELINE 2 — ITEM–BASED COLLABORATIVE FILTERING"
      ],
      "metadata": {
        "id": "TIp-Nb8lG_qR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "# Build sparse interaction matrix\n",
        "rows = train_df['user_idx']\n",
        "cols = train_df['item_idx']\n",
        "data = np.ones(len(train_df))\n",
        "\n",
        "interaction_matrix = csr_matrix((data, (rows, cols)), shape=(num_users, num_items))\n",
        "\n",
        "# Compute item-item cosine similarity\n",
        "item_sim = cosine_similarity(interaction_matrix.T)\n",
        "\n",
        "print(\"Item similarity matrix:\", item_sim.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lW0XvPfd8Rmv",
        "outputId": "50e89dec-c886-442b-97e4-0c59d90c11ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Item similarity matrix: (85, 85)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend_itemcf(user_id, K=10):\n",
        "    user_vector = interaction_matrix[user_id].toarray().flatten()\n",
        "\n",
        "    # Score = weighted similarity sum\n",
        "    scores = item_sim.dot(user_vector)\n",
        "\n",
        "    # Remove already seen items\n",
        "    scores[user_vector > 0] = -1\n",
        "\n",
        "    # Top-K\n",
        "    top_items = scores.argsort()[::-1][:K]\n",
        "    return top_items\n"
      ],
      "metadata": {
        "id": "m6yOJp9HHDCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_itemcf(K=10):\n",
        "    hits = 0\n",
        "    total = len(test_df)\n",
        "    ndcg_sum = 0\n",
        "\n",
        "    for _, row in test_df.iterrows():\n",
        "        user = row.user_idx\n",
        "        true_item = row.item_idx\n",
        "\n",
        "        recs = recommend_itemcf(user, K)\n",
        "\n",
        "        if true_item in recs:\n",
        "            hits += 1\n",
        "            rank = list(recs).index(true_item)\n",
        "            ndcg_sum += 1 / np.log2(rank + 2)\n",
        "\n",
        "    precision = hits / total\n",
        "    recall = hits / total\n",
        "    accuracy = hits / total\n",
        "    ndcg = ndcg_sum / total\n",
        "\n",
        "    return precision, recall, accuracy, ndcg\n"
      ],
      "metadata": {
        "id": "58Hbj6A3HGW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p, r, acc, n = evaluate_itemcf(K=10)\n",
        "\n",
        "print(\"===== ITEM-BASED CF (BASELINE 2) =====\")\n",
        "print(\"Precision@10:\", p)\n",
        "print(\"Recall@10:\", r)\n",
        "print(\"Accuracy@10:\", acc)\n",
        "print(\"NDCG@10:\", n)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vw2YKfRHIhz",
        "outputId": "6592552a-63bb-48d7-d519-27e56c527931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== ITEM-BASED CF (BASELINE 2) =====\n",
            "Precision@10: 0.4106962663975782\n",
            "Recall@10: 0.4106962663975782\n",
            "Accuracy@10: 0.4106962663975782\n",
            "NDCG@10: 0.39089969315564355\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BASELINE 3 — ALS MATRIX FACTORIZATION"
      ],
      "metadata": {
        "id": "yVjNH6HOHvcj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BASELINE 1: POPULARITY RECOMMENDER"
      ],
      "metadata": {
        "id": "S9XLzR-ZLfMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Compute item popularity from train_df\n",
        "item_popularity = train_df['item_idx'].value_counts().index.tolist()\n",
        "\n",
        "def evaluate_popularity(K=10):\n",
        "    precisions, recalls, accs, ndcgs = [], [], [], []\n",
        "\n",
        "    top_k_items = item_popularity[:K]\n",
        "\n",
        "    for user in test_df['user_idx'].unique():\n",
        "        true_items = test_df[test_df.user_idx == user].item_idx.values.tolist()\n",
        "\n",
        "        hits = len(set(top_k_items) & set(true_items))\n",
        "\n",
        "        precision = hits / K\n",
        "        recall = hits / len(true_items)\n",
        "        accuracy = 1 if hits > 0 else 0\n",
        "        ndcg = hits / K  # simplified ndcg\n",
        "\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "        accs.append(accuracy)\n",
        "        ndcgs.append(ndcg)\n",
        "\n",
        "    return (\n",
        "        np.mean(precisions),\n",
        "        np.mean(recalls),\n",
        "        np.mean(accs),\n",
        "        np.mean(ndcgs)\n",
        "    )\n",
        "\n",
        "# Run\n",
        "p, r, a, n = evaluate_popularity()\n",
        "\n",
        "print(\"\\n===== POPULARITY RECOMMENDER (BASELINE 2) =====\")\n",
        "print(\"Precision@10:\", p)\n",
        "print(\"Recall@10:   \", r)\n",
        "print(\"Accuracy@10: \", a)\n",
        "print(\"NDCG@10:     \", n)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvYbglCgQtnQ",
        "outputId": "599cfee9-9403-47b5-d979-a3fba758cacf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== POPULARITY RECOMMENDER (BASELINE 2) =====\n",
            "Precision@10: 0.09253279515640768\n",
            "Recall@10:    0.9253279515640767\n",
            "Accuracy@10:  0.9253279515640767\n",
            "NDCG@10:      0.09253279515640768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BASELINE 2: ITEM-ITEM COSINE SIMILARITY (KNN CF)"
      ],
      "metadata": {
        "id": "aknIr6D_Lxfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "# Build user-item matrix from train_df\n",
        "rows = train_df['user_idx']\n",
        "cols = train_df['item_idx']\n",
        "data = np.ones(len(train_df))\n",
        "train_matrix = csr_matrix((data, (rows, cols)), shape=(num_users, num_items))\n",
        "\n",
        "# Compute item-item similarity\n",
        "item_sim = cosine_similarity(train_matrix.T)   # 85 x 85\n",
        "\n",
        "def recommend_knn(user, K):\n",
        "    user_items = train_matrix[user].indices  # items user interacted with\n",
        "\n",
        "    if len(user_items) == 0:\n",
        "        return item_popularity[:K]  # fallback\n",
        "\n",
        "    # Sum similarity scores from all items user interacted with\n",
        "    scores = np.sum(item_sim[user_items], axis=0)\n",
        "\n",
        "    # Remove already seen items\n",
        "    scores[user_items] = -1\n",
        "\n",
        "    top_k = np.argsort(scores)[::-1][:K]\n",
        "    return top_k.tolist()\n",
        "\n",
        "def evaluate_knn(K=10):\n",
        "    precisions, recalls, accs, ndcgs = [], [], [], []\n",
        "\n",
        "    for user in test_df['user_idx'].unique():\n",
        "        true_items = test_df[test_df.user_idx == user].item_idx.values.tolist()\n",
        "\n",
        "        pred_items = recommend_knn(user, K)\n",
        "\n",
        "        hits = len(set(pred_items) & set(true_items))\n",
        "\n",
        "        precision = hits / K\n",
        "        recall = hits / len(true_items)\n",
        "        accuracy = 1 if hits > 0 else 0\n",
        "        ndcg = hits / K\n",
        "\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "        accs.append(accuracy)\n",
        "        ndcgs.append(ndcg)\n",
        "\n",
        "    return (\n",
        "        np.mean(precisions),\n",
        "        np.mean(recalls),\n",
        "        np.mean(accs),\n",
        "        np.mean(ndcgs)\n",
        "    )\n",
        "\n",
        "# Run\n",
        "p, r, a, n = evaluate_knn(K=10)\n",
        "\n",
        "print(\"\\n===== ITEM-ITEM KNN COLLABORATIVE FILTERING (BASELINE 3) =====\")\n",
        "print(\"Precision@10:\", p)\n",
        "print(\"Recall@10:   \", r)\n",
        "print(\"Accuracy@10: \", a)\n",
        "print(\"NDCG@10:     \", n)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-hjSSKaLh6M",
        "outputId": "b4073558-2a3a-4ebd-b195-a78539c2ec57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== ITEM-ITEM KNN COLLABORATIVE FILTERING (BASELINE 3) =====\n",
            "Precision@10: 0.04096871846619576\n",
            "Recall@10:    0.40968718466195764\n",
            "Accuracy@10:  0.40968718466195764\n",
            "NDCG@10:      0.04096871846619576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kuCYqLnBL0jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Baseline Code\n"
      ],
      "metadata": {
        "id": "8FVHiLjeWyVg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BASELINE 4 — ITEM–ITEM SIMILARITY (COSINE SIMILARITY)"
      ],
      "metadata": {
        "id": "hZE2bj-PM2Js"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ========== BUILD USER–ITEM MATRIX ==========\n",
        "num_users = df.user_idx.nunique()\n",
        "num_items = df.item_idx.nunique()\n",
        "\n",
        "train_matrix = csr_matrix(\n",
        "    (np.ones(len(train_df)), (train_df.user_idx, train_df.item_idx)),\n",
        "    shape=(num_users, num_items)\n",
        ")\n",
        "\n",
        "# ========== ITEM–ITEM COSINE SIMILARITY ==========\n",
        "print(\"Computing item-item similarity matrix...\")\n",
        "item_sim = cosine_similarity(train_matrix.T)  # shape: items × items\n",
        "\n",
        "print(\"Item similarity matrix ready:\", item_sim.shape)\n",
        "\n",
        "# ========== RECOMMENDATION FUNCTION ==========\n",
        "def recommend_items(user_id, K=10):\n",
        "    # items the user already interacted with\n",
        "    user_items = train_matrix[user_id].indices\n",
        "\n",
        "    if len(user_items) == 0:\n",
        "        return []  # cold start\n",
        "\n",
        "    # compute score = sum of similarities for all items user interacted with\n",
        "    scores = item_sim[user_items].sum(axis=0)\n",
        "\n",
        "    # remove items already seen\n",
        "    scores[user_items] = -1e9\n",
        "\n",
        "    # top K item indices\n",
        "    top_items = np.argsort(scores)[-K:][::-1]\n",
        "    return top_items.tolist()\n",
        "\n",
        "# ========== METRIC FUNCTIONS ==========\n",
        "def precision_at_k(pred, true, k):\n",
        "    return len(set(pred[:k]) & set(true)) / k\n",
        "\n",
        "def recall_at_k(pred, true, k):\n",
        "    return len(set(pred[:k]) & set(true)) / len(true) if len(true) > 0 else 0\n",
        "\n",
        "def accuracy_at_k(pred, true):\n",
        "    return 1 if set(pred) & set(true) else 0\n",
        "\n",
        "def ndcg_at_k(pred, true, k):\n",
        "    dcg = 0.0\n",
        "    for i, p in enumerate(pred[:k]):\n",
        "        if p in true:\n",
        "            dcg += 1 / np.log2(i + 2)\n",
        "    idcg = 1.0  # because 1 relevant item per user\n",
        "    return dcg / idcg\n",
        "\n",
        "# ========== EVALUATION ==========\n",
        "def evaluate_item_item(K=10):\n",
        "    precisions, recalls, accuracies, ndcgs = [], [], [], []\n",
        "\n",
        "    print(\"\\nEvaluating Item–Item Collaborative Filtering...\")\n",
        "    for user in tqdm(test_df.user_idx.unique()):\n",
        "\n",
        "        pred_items = recommend_items(user, K)\n",
        "        true_items = test_df[test_df.user_idx == user].item_idx.values.tolist()\n",
        "\n",
        "        if len(pred_items) == 0:\n",
        "            continue\n",
        "\n",
        "        precisions.append(precision_at_k(pred_items, true_items, K))\n",
        "        recalls.append(recall_at_k(pred_items, true_items, K))\n",
        "        accuracies.append(accuracy_at_k(pred_items, true_items))\n",
        "        ndcgs.append(ndcg_at_k(pred_items, true_items, K))\n",
        "\n",
        "    return (\n",
        "        np.mean(precisions),\n",
        "        np.mean(recalls),\n",
        "        np.mean(accuracies),\n",
        "        np.mean(ndcgs)\n",
        "    )\n",
        "\n",
        "# RUN BASELINE 4\n",
        "p, r, a, n = evaluate_item_item(K=10)\n",
        "\n",
        "print(\"\\n===== BASELINE 4: ITEM-ITEM COLLABORATIVE FILTERING =====\")\n",
        "print(\"Precision@10:\", p)\n",
        "print(\"Recall@10:   \", r)\n",
        "print(\"Accuracy@10: \", a)\n",
        "print(\"NDCG@10:     \", n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbvqS16KMGsm",
        "outputId": "f434bee7-27a8-48ff-ae6e-d0b905aa22f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing item-item similarity matrix...\n",
            "Item similarity matrix ready: (85, 85)\n",
            "\n",
            "Evaluating Item–Item Collaborative Filtering...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 991/991 [00:00<00:00, 1869.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== BASELINE 4: ITEM-ITEM COLLABORATIVE FILTERING =====\n",
            "Precision@10: 0.04096871846619576\n",
            "Recall@10:    0.40968718466195764\n",
            "Accuracy@10:  0.40968718466195764\n",
            "NDCG@10:      0.3922410305512074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xmBENY_DQEOX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}